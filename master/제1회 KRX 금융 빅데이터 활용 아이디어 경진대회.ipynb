{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf23d1af",
   "metadata": {},
   "source": [
    "# 제1회 KRX 금융 빅데이터 활용 아이디어 경진대회"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e87072",
   "metadata": {},
   "source": [
    "## 주제 : 개인투자자의 ktop30 감정지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1331fca",
   "metadata": {},
   "source": [
    "- __chapter 1. 정형데이터__\n",
    "<br/>\n",
    "- __chapter 2. 비정형 데이터__\n",
    "    - 비정형 데이터 크롤링\n",
    "        - 유튜브댓글 api\n",
    "        - 네이버금융 종목토론방 댓글 크롤링\n",
    "    - 비정형 데이터 전처리\n",
    "        - 댓글 전처리\n",
    "        - 댓글 토크나이징\n",
    "        - 댓글 레이블링\n",
    "        - 학습데이터 생성 (댓글 도미넌스 기반 랜덤추출)\n",
    "        - 날짜별 데이터 정리\n",
    "        <br/>\n",
    "- __chapter 3. BERT 모델링__\n",
    "    - 모델 학습\n",
    "    - 모델 예측\n",
    "    <br/>\n",
    "- __chapter 4. 시각화 및 서비스__\n",
    "\n",
    "<br/>\n",
    "- __chapter 5. Reference__\n",
    "    - [한국거래소](http://www.krx.co.kr/main/main.jsp)\n",
    "    - [네이버 금융](https://finance.naver.com/)\n",
    "    - [CNN Fear & Greed Index](https://edition.cnn.com/markets/fear-and-greed) -> 정형지표만 이용한듯 해요\n",
    "    - [Crypto Fear & Greed Index](https://alternative.me/crypto/fear-and-greed-index/)\n",
    "    - 단어집 위치\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e841c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3d17f",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "    금융시장은 투자자의 시장인식에 따라 영향이 있을것이라 생각했습니다. 국내에서는 심리를 대변하는 지수로 정형지표의 계산으로 산출된 VIX지수를 지표로 이용합니다. 그러나 Crypto Fear & Greed Index 에서 비정형 데이터로 트위터 데이터를 산출식에 포함시키는 시도가 있었습니다. (가이드글)\n",
    "\n",
    "    # 개인투자자의 시장인식이 댓글형태로 나타나는 네이버 종목토론실 댓글을 이용합니다. # 개인투자자의 시장인식을 수치화시켜 하나의 지표로 만들었습니다. # KODEX30 으로 프로토타입을 작성했습니다.\n",
    "\n",
    "    인덱스가 FEAR(0)에 가까울수록 투자자가 느끼는 시장인식은 가격이 하락하고 있음을 대변합니다. 인덱스 활용방법으로는 매수시점을 정하는데 도움을 받을 수 있습니다. 반대의 경우 가격이 상승하는것을 대변하고, 매도시점을 정하는데 도움을 받을 수 있습니다.\n",
    "\n",
    "\n",
    "- 사용데이터소개 (비정형/정형)\n",
    "- 인덱스의 기대효과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c706e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d44db",
   "metadata": {},
   "source": [
    "## chapter 1. 정형 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b34f4",
   "metadata": {},
   "source": [
    "## chapter 2. 비정형 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d604ed5",
   "metadata": {},
   "source": [
    "### 2.1 비정형 데이터 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b669a",
   "metadata": {},
   "source": [
    "#### 2.1.1 유튜브댓글 api\n",
    "- 유튜브 채널 '삼프로tv'에서 2020년 06월부터 2022년 06월 사이에 업로드된 영상의 댓글을 크롤링하였습니다.\n",
    "- '영상 제목', '영상 업로드 날짜', '영상 아이디', '댓글 내용', '댓글 작성자', '댓글 작성 날짜', '좋아요 수'에 대한 정보를 가져왔습니다.\n",
    "- 국내 시장에 대한 반응을 확인하기 위해 해외 시장에 관련된 영상이나 경제와 관련없는 영상에 달린 댓글들은 제외하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d72e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import scrapetube\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3823ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'api_key'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "search_response = youtube.search().list(\n",
    "    q = '삼프로tv',\n",
    "    order = 'relevance',\n",
    "    part = 'snippet',\n",
    "    maxResults = 10\n",
    "    ).execute()\n",
    "\n",
    "channel_id = search_response['items'][0]['snippet']['channelId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a78f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = scrapetube.get_channel(channel_id)\n",
    "\n",
    "video_ids = []\n",
    "\n",
    "for video in videos:\n",
    "    video_ids.append(video['videoId'])\n",
    "    \n",
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_infos = []\n",
    "\n",
    "for i in range(800):\n",
    "    start = i * 50\n",
    "    end = (i + 1) * 50\n",
    "    video_request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=','.join(video_ids[start:end]))\n",
    "\n",
    "    video_response = video_request.execute()\n",
    "\n",
    "    for item in video_response['items']:\n",
    "        title = item['snippet']['title']\n",
    "        if ('글로벌 이슈체크' in title) or ('글로벌 마켓브리핑' in title) or ('직장인 vlog' in title):\n",
    "            continue\n",
    "        video_infos.append([item['snippet']['title'], item['snippet']['publishedAt'], item['id']])\n",
    "        \n",
    "df = pd.DataFrame(video_infos, columns=['title', 'video_date', 'id'])\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe46834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df = df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9347d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_22 = df[(pd.DatetimeIndex(df.index).year == 2022) & (pd.DatetimeIndex(df.index).month <= 6)]\n",
    "df_21 = df[(pd.DatetimeIndex(df.index).year == 2021)]\n",
    "df_20 = df[(pd.DatetimeIndex(df.index).year == 2020) & (pd.DatetimeIndex(df.index).month >= 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_22 = []\n",
    "\n",
    "for video_id in df_22['id']:\n",
    "    api_obj = build('youtube', 'v3', developerKey=api_key)\n",
    "    response = api_obj.commentThreads().list(part='snippet', videoId=video_id, maxResults=100).execute()\n",
    " \n",
    "    while response:\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments_22.append([video_id, comment['textDisplay'], comment['authorDisplayName'], comment['publishedAt'], comment['likeCount']])\n",
    " \n",
    "        if 'nextPageToken' in response:\n",
    "            response = api_obj.commentThreads().list(part='snippet', videoId=video_id, pageToken=response['nextPageToken'], maxResults=100).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "df1 = pd.DataFrame(comments_22, columns=['id', 'comment', 'author', 'comment_date', 'num_likes'])\n",
    "print(len(df1))\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_21 = []\n",
    "\n",
    "for video_id in df_21['id']:\n",
    "    api_obj = build('youtube', 'v3', developerKey=api_key)\n",
    "    response = api_obj.commentThreads().list(part='snippet', videoId=video_id, maxResults=100).execute()\n",
    " \n",
    "    while response:\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments_21.append([video_id, comment['textDisplay'], comment['authorDisplayName'], comment['publishedAt'], comment['likeCount']])\n",
    "\n",
    "        if 'nextPageToken' in response:\n",
    "            response = api_obj.commentThreads().list(part='snippet', videoId=video_id, pageToken=response['nextPageToken'], maxResults=100).execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "df2 = pd.DataFrame(comments_21, columns=['id', 'comment', 'author', 'comment_date', 'num_likes'])\n",
    "print(len(df2))\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03981cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_20 = []\n",
    "\n",
    "for video_id in df_20['id']:\n",
    "    api_obj = build('youtube', 'v3', developerKey=api_key)\n",
    "    response = api_obj.commentThreads().list(part='snippet', videoId=video_id, maxResults=100).execute()\n",
    " \n",
    "    while response:\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments_20.append([video_id, comment['textDisplay'], comment['authorDisplayName'], comment['publishedAt'], comment['likeCount']])\n",
    " \n",
    "        if 'nextPageToken' in response:\n",
    "            response = api_obj.commentThreads().list(part='snippet', videoId=video_id, pageToken=response['nextPageToken'], maxResults=100).execute()\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "df3 = pd.DataFrame(comments_21, columns=['id', 'comment', 'author', 'comment_date', 'num_likes'])\n",
    "print(len(df3))\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_22 = df_22.reset_index()\n",
    "df_21 = df_21.reset_index()\n",
    "df_20 = df_20.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6260bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampro_22 = pd.merge(df_22, df1, on='id', how='outer')\n",
    "sampro_21 = pd.merge(df_21, df2, on='id', how='outer')\n",
    "sampro_20 = pd.merge(df_20, df3, on='id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b19ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampro_22.to_csv('./sampro/sampro_22.csv', index=False)\n",
    "sampro_21.to_csv('./sampro/sampro_21.csv', index=False)\n",
    "sampro_20.to_csv('./sampro/sampro_20.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampro_2year = pd.concat([sampro_22, sampro_21, sampro_20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fee961",
   "metadata": {},
   "source": [
    "#### 2.1.2 종목토론실 댓글 크롤링\n",
    "- 종목토론실 댓글에서 '날짜', '댓글제목', '조회수', '좋아요', 싫어요' 를 크롤링 했습니다.\n",
    "- 기업대상은 KODEX30의 기업입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045505b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38706b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code(symbol):\n",
    "    krx = pd.read_csv('./krx_code.csv',encoding='utf-8')\n",
    "    krx = krx.set_index('한글 종목약명')\n",
    "    try:\n",
    "        code = krx.at[symbol,'단축코드']\n",
    "        return code\n",
    "    except:\n",
    "        print('종목명을 다시 확인해주세요.')\n",
    "        return 0\n",
    "\n",
    "def get_comment_csv(symbol,page,year,month,day):   \n",
    "    code = get_code(symbol)\n",
    "    date_list = [] # 날짜\n",
    "    comment_list = [] # 댓글\n",
    "    view_list = [] # 조회수\n",
    "    good_list = [] # 좋아요\n",
    "    bad_list = [] # 싫어요\n",
    "    flag = 0\n",
    "    for i in range(1,page+1):\n",
    "        url = f'https://finance.naver.com/item/board.naver?code={code}&page={i}'\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36 Edg/100.0.1185.50'}\n",
    "        res = requests.get(url, headers = headers)\n",
    "        bs = BeautifulSoup(res.text, 'html.parser')\n",
    "        for j in range(20):\n",
    "            try:\n",
    "                root = bs.find('div',{'class':'section inner_sub'}).find_all('tr',{'onmouseover':'mouseOver(this)'})[j].text.split('\\n')\n",
    "                \n",
    "                date_list.append(root[1].replace('.','-'))\n",
    "                \n",
    "                if len(root) == 14: # 답글\n",
    "                    comment_list.append('답글:'+root[4])\n",
    "                    view_list.append(root[10])\n",
    "                    good_list.append(root[11])\n",
    "                    bad_list.append(root[12])          \n",
    "                elif len(root) == 13: # 기본\n",
    "                    comment_list.append(root[3])\n",
    "                    view_list.append(root[9])\n",
    "                    good_list.append(root[10])\n",
    "                    bad_list.append(root[11])\n",
    "                else: # 에러\n",
    "                    comment_list.append('error')\n",
    "                    view_list.append(0)\n",
    "                    good_list.append(0)\n",
    "                    bad_list.append(0)   \n",
    "            except:\n",
    "                break\n",
    "#                 date_list.append('error')\n",
    "#                 comment_list.append('error')\n",
    "#                 view_list.append(0)\n",
    "#                 good_list.append(0)\n",
    "#                 bad_list.append(0)  \n",
    "            tp = [int(j) for j in root[1].split()[0].split('.')]\n",
    "            if dt.datetime(tp[0],tp[1],tp[2]) < dt.datetime(year,month,day):\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "        print(f'\\r{i}페이지 크롤링 완료.',end='')\n",
    "        \n",
    "#         for i in date_list:\n",
    "#             tp = [int(j) for j in i.split()[0].split('-')]\n",
    "#             if dt.datetime(tp[0],tp[1],tp[2]) < dt.datetime(year,month,day):\n",
    "#                 flag = 1\n",
    "#                 break\n",
    "#         if flag == 1:\n",
    "#             break\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['날짜'] = date_list\n",
    "    df['댓글'] = comment_list\n",
    "    df['조회수'] = view_list\n",
    "    df['좋아요'] = good_list\n",
    "    df['싫어요'] = bad_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b68a02",
   "metadata": {},
   "source": [
    "- ktop30 리스트 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('./KODEX_KTOP_30_20220629.xlsx',header=2).drop(0,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "ktop30_company = pd.read_excel('./KODEX_KTOP_30_20220629.xlsx',header=2).drop(0,axis=0)['종목명']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c5777",
   "metadata": {},
   "source": [
    "- 샘플 실행결과 입니다. \n",
    "- 인풋 파라미터는 기업이름,최대 페이지수,년,월,일 입니다. 설정한 년/월/일 이후의 댓글이 나오면 크롤링을 멈추게 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_comment_csv('미래에셋증권',500,2022,6,1) # sample\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 건너뛰기\n",
    "for i in range(3):\n",
    "    df = get_comment_csv(list(ktop30_company)[i],100000,2020,6,1)\n",
    "    #df = df[df['날짜']=='2022-07-01']\n",
    "    df.to_csv(f'./src/2year_time/{list(ktop30_company)[i]}_2year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9112c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링된 데이터 확인\n",
    "pd.read_csv('./src/year_time/삼성전자_year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dde1dd",
   "metadata": {},
   "source": [
    "### 2.2 비정형 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842532a",
   "metadata": {},
   "source": [
    "#### 2.2.1 댓글 전처리\n",
    "- 온전한 한글문자 이외에 모든 부분을 제거했습니다.\n",
    "- 초성,알파벳,특수문자,이모티콘 등이 제거 되었습니다.\n",
    "\n",
    "\n",
    "- 예시 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804091bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv('./src/2year_time/삼성화재_2year.csv')\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sents_df(company):\n",
    "    try:\n",
    "        target = pd.read_csv(f'./src/{company}_2year.csv',encoding='utf8').drop('Unnamed: 0',axis=1)\n",
    "    except:\n",
    "        target = pd.read_csv(f'./src/{company}_2year.csv',encoding='utf8')\n",
    "\n",
    "    if company == 'sampro':\n",
    "        target.rename(columns={'comment':'댓글'},inplace=True)\n",
    "    df = target\n",
    "    df['정제된 댓글'] = df['댓글'].str.replace('\\\\[삭제된 게시물의 답글\\\\]',' ')\n",
    "    df['정제된 댓글'] = df['정제된 댓글'].str.replace('답글:',' ')\n",
    "    df['정제된 댓글'] = df['정제된 댓글'].str.replace('[^가-힣]',' ').str.replace(' +',' ').str.strip()\n",
    "    df = df[df['정제된 댓글'] != '']\n",
    "    df = df.reset_index(drop=True)\n",
    "    return  df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632e0c8",
   "metadata": {},
   "source": [
    "#### 2.2.2 댓글 토크나이징\n",
    "\n",
    "- Bert 모델링을 위한 labeling 사전작업 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c36608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_save(company):\n",
    "    df = clean_sents_df(company)\n",
    "    df['정제된 댓글 길이'] = [len(str(i)) for i in df['정제된 댓글']]\n",
    "    df = df[df['정제된 댓글 길이'] > 5]\n",
    "\n",
    "    tp = [str(i) for i in list(df['정제된 댓글'])]\n",
    "    save = '\\n'.join(tp)\n",
    "    f = open(\"./corpus_target.txt\", 'a',encoding='utf8')\n",
    "    f.write(save)\n",
    "    f.close()\n",
    "    \n",
    "def corpus_init():\n",
    "    ktop30_company = pd.read_excel('./src/KODEX_KTOP_30_20220629.xlsx',header=2).drop(0,axis=0)['종목명']\n",
    "    company_set = list(ktop30_company)\n",
    "    company_set.append('sampro')\n",
    "    f = open(\"./corpus_target.txt\", 'w',encoding='utf8')\n",
    "    f.write('')\n",
    "    f.close()\n",
    "    for company in company_set:\n",
    "        corpus_save(company)\n",
    "\n",
    "def return_tokenizer():\n",
    "    corpus = DoublespaceLineCorpus(\"./src/corpus_target.txt\",iter_sent=True)\n",
    "    noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "    nouns = noun_extractor.train_extract(corpus)\n",
    "    scores = {word:score.score for word, score in nouns.items()}\n",
    "    tokenizer = LTokenizer(scores=scores)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_init()\n",
    "tokenizer = return_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0289c97",
   "metadata": {},
   "source": [
    "- 3181878 개의 네이버 종목토론방, 유튜브 댓글을 학습시키고, 학습된 tokenizer를 이용하여 전처리 댓글을 토큰화 시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = clean_sents_df('삼성전자')\n",
    "target_df['토큰화 댓글'] = [tokenizer(str(i)) for i in target_df['정제된 댓글']]\n",
    "target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765422c1",
   "metadata": {},
   "source": [
    "#### 2.2.3 댓글 레이블링\n",
    "- fear_words_set 과 greed_words_set을 설정하고 토크나이징댓글의 요소가 fear 단어집에 있으면 -1씩 greed 단어집에 있으면 +1씩 부여했습니다.\n",
    "- 양수인경우 greed, 음수인경우 fear로 labeling 했습니다. 0 인경우 일단 데이터프레임에 기록되고, 최종적으로 train_data 선정시에는 제외됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(target_df):\n",
    "    \n",
    "    f = open(\"./neg_pol_word.txt\", 'r',encoding='utf8')\n",
    "    words = f.readlines()\n",
    "    f.close()\n",
    "    fear_words_set = {word.strip('\\n') for word in words}\n",
    "\n",
    "    f = open(\"./pos_pol_word.txt\", 'r',encoding='utf8')\n",
    "    words = f.readlines()\n",
    "    f.close()\n",
    "    greed_words_set = {word.strip('\\n') for word in words}\n",
    "    \n",
    "    label_score = []\n",
    "    for token_list in target_df['토큰화 댓글']:\n",
    "        sent_score = 0\n",
    "        for token in token_list:\n",
    "            if token in fear_words_set:\n",
    "                sent_score -= 1\n",
    "            elif token in greed_words_set:\n",
    "                sent_score += 1\n",
    "\n",
    "        if sent_score < 0:\n",
    "            label_score.append(-1)\n",
    "        elif sent_score > 0:\n",
    "            label_score.append(1)\n",
    "        else:\n",
    "            label_score.append(0)\n",
    "            \n",
    "    target_df['label'] = label_score\n",
    "    \n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142d956",
   "metadata": {},
   "source": [
    "#### 2.2.4 학습데이터 생성\n",
    "\n",
    "- 학습데이터는 종목토론실 댓글에서 3만개, 유튜브 댓글에서 1만개 랜덤추출 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting_train_data():\n",
    "    company = list(pd.read_excel('./KODEX_KTOP_30_20220715.xls',header=2)['종목명'][1:])\n",
    "    dominance = list(pd.read_excel('./KODEX_KTOP_30_20220715.xls',header=2)['비중(%)'][1:])\n",
    "\n",
    "    train_data = pd.DataFrame(columns=['날짜','댓글','조회수','좋아요','싫어요','정제된 댓글','토큰화 댓글','label'])\n",
    "    for idx in range(30):\n",
    "        target_df = clean_sents_df(company[idx])\n",
    "        target_df['토큰화 댓글'] = [tokenizer(str(i)) for i in target_df['정제된 댓글']]\n",
    "        \n",
    "        label_df = labeling(target_df)\n",
    "        \n",
    "        label_df = label_df[label_df['label'] != 0] # except label:0 \n",
    "        label_df = label_df.sample(int(30000*dominance[idx])+1,replace=True) # replace option : 샘플수가 부족할경우 중복이 생김 ** 추후 수정\n",
    "        train_data = train_data.append(label_df)\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = setting_train_data()\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data.drop('index',axis=1,inplace=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삼프로\n",
    "target_df = clean_sents_df('sampro')\n",
    "target_df['토큰화 댓글'] = [tokenizer(str(i)) for i in target_df['정제된 댓글']]\n",
    "label_df = labeling(target_df)\n",
    "\n",
    "label_df = label_df[label_df['label'] != 0] # except label:0 \n",
    "label_df = label_df.sample(10000,replace=True) # replace option : 샘플수가 부족할경우 중복이 생김 ** 추후 수정\n",
    "train_data = train_data.append(label_df)\n",
    "\n",
    "train_data = train_data.loc[:,['정제된 댓글','label']].reset_index().drop('index',axis=1)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8998f4",
   "metadata": {},
   "source": [
    "#### 2.2.5 날짜별 데이터 정리\n",
    "+ krx30 종목토론방 댓글, 유튜브 댓글을 각 날짜별로 정리한 후, 2022-06-01 ~ 2022-06-30 한 달의 데이터를 예측에 사용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 이름 불러오기\n",
    "ktop30_company = pd.read_excel('KODEX_KTOP_30_20220629.xlsx',header=2).drop(0,axis=0)['종목명']\n",
    "company_set = list(ktop30_company)\n",
    "company_set.append('sampro1')\n",
    "company_set.append('sampro2')\n",
    "\n",
    "# 모든 데이터를 하나의 리스트에 저장\n",
    "alldf = []\n",
    "for company in company_set:\n",
    "    df = pd.read_csv(company + \".csv\")\n",
    "    if \"sampro\" in company:\n",
    "        df2 = df[[\"video_date\",\"댓글\",\"정제된 댓글\"]]\n",
    "        df2.columns = [\"날짜\",\"댓글\",\"정제된 댓글\"]\n",
    "    else:\n",
    "        df2 = df[[\"날짜\",\"댓글\",\"정제된 댓글\"]]\n",
    "    alldf.append(df2)\n",
    "\n",
    "# 날짜 형식을 년-월-일 로 \n",
    "for df in alldf:\n",
    "    date_list = []\n",
    "    print(len(df[\"날짜\"]))\n",
    "    for i in range(len(df[\"날짜\"])):\n",
    "        date_list.append(df[\"날짜\"][i][:10])\n",
    "    df[\"날짜\"] = date_list\n",
    "\n",
    "# 2022-06-01 ~ 2022-06-30 날짜 데이터를 각각 하나로 뭉침\n",
    "target_date = ['2022-06-01', '2022-06-02', '2022-06-03', '2022-06-04', '2022-06-05',\n",
    "               '2022-06-06', '2022-06-07', '2022-06-08', '2022-06-09', '2022-06-10',\n",
    "               '2022-06-11', '2022-06-12', '2022-06-13', '2022-06-14', '2022-06-15',\n",
    "               '2022-06-16', '2022-06-17', '2022-06-18', '2022-06-19', '2022-06-20',\n",
    "               '2022-06-21', '2022-06-22', '2022-06-23', '2022-06-24', '2022-06-25',\n",
    "               '2022-06-26', '2022-06-27', '2022-06-28', '2022-06-29', '2022-06-30',\n",
    "               '2022-05-31']\n",
    "td = sorted(target_date,reverse=True)\n",
    "all_date = []\n",
    "for df in alldf:\n",
    "    date_idx = dict()\n",
    "    for i in range(len(df[\"날짜\"])):\n",
    "        if df[\"날짜\"][i] in date_idx:\n",
    "            continue\n",
    "        else:\n",
    "            if df[\"날짜\"][i] in target_date:\n",
    "                date_idx[df[\"날짜\"][i]] = i\n",
    "    all_date.append(date_idx)\n",
    "for j in all_date:\n",
    "    for i in range(len(td)):\n",
    "        if td[i] in j:\n",
    "            continue\n",
    "        else:\n",
    "            j[td[i]] = 0\n",
    "for i in all_date:\n",
    "    if i[\"2022-05-31\"] == 0:\n",
    "        continue\n",
    "    for j in range(len(td)-2,0,-1):\n",
    "        if i[td[j]] == 0:\n",
    "            i[td[j]] = i[td[j+1]]\n",
    "for i in range(30):\n",
    "    print(i)\n",
    "    for c in range(32):\n",
    "        print(\"c = \",c)\n",
    "        df = alldf[c]\n",
    "        date_idx = all_date[c]\n",
    "        if c == 0:\n",
    "            df_day = df.iloc[date_idx[td[i]]:date_idx[td[i+1]]]\n",
    "        elif c == 30:\n",
    "            continue\n",
    "        else:\n",
    "            df_today = df.iloc[date_idx[td[i]]:date_idx[td[i+1]]]\n",
    "            df_day = pd.concat([df_day,df_today])\n",
    "    df_day.to_csv(td[i] + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d72d56",
   "metadata": {},
   "source": [
    "## chapter 3. Bert 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6639d",
   "metadata": {},
   "source": [
    "### 3.1 모델 학습\n",
    "+ 2.2.4에서 생성된 학습 데이터를 모델 학습에 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15041306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befa005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 데이터 적재\n",
    "file_path = \"train_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 댓글과 라벨만 사용, 훈련 데이터와 테스트 데이터 나눔\n",
    "df_nn = df[[\"정제된 댓글\",\"label\"]]\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_nn[\"정제된 댓글\"],df_nn[\"label\"],test_size=0.2,random_state=11)\n",
    "df_train = pd.concat([x_train,y_train],axis = 1)\n",
    "df_test = pd.concat([x_test,y_test],axis = 1)\n",
    "print(df_train[\"label\"].unique())\n",
    "df_train.head()\n",
    "\n",
    "# 데이터셋을 koBERT 모델에 맞게 변형하는 클래스\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i]) for i in dataset[sent_idx]]\n",
    "        self.labels = [np.int32(i) for i in dataset[label_idx]]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))  \n",
    "\n",
    "\n",
    "# koBERT 모듈을 적재하여 모델을 학습시키는 분류기 \n",
    "class BERTClassifier(nn.Module)\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=0.5,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# 파라미터 설정 \n",
    "max_len = 128\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 3\n",
    "max_grad_norm = 1\n",
    "log_interval = 100\n",
    "learning_rate =  0.0001\n",
    "\n",
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "# 훈련데이터, 테스트데이터 전처리\n",
    "data_train = BERTDataset(df_train, \"정제된 댓글\", \"label\", tok, max_len, True, False)\n",
    "data_test = BERTDataset(df_test, \"정제된 댓글\", \"label\", tok, max_len, True, False)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "# 모델 생성\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "\n",
    "# optimizer 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 모델 학습\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "train_dataloader\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model, '/KoBERT_model_dr_0.5.pt') \n",
    "torch.save(model.state_dict(), '/model_state_dict_dr_0.5.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec11b34",
   "metadata": {},
   "source": [
    "### 3.2 데이터 예측\n",
    "+ 2.2.5에서 생성된 2022-06-01 ~ 2022-06-30 댓글 데이터를 저장된 모델을 사용하여 fear/greed 점수를 예측하였습니다.\n",
    "+ BERT의 감성분석 모델을 사용하였으며, 각각의 댓글은 fear/greed로 예측됩니다.<br/>\n",
    "이때, 예측된 fear/greed는 각각 점수를 가지고 있으며, 이 점수로 각 댓글의 fear/greed 점수를 산출합니다.\n",
    "+ 산출식은 |fear score - greed score| x (-1 or 1) 입니다.<br/>\n",
    "여기서 -1과 1을 판단하는 기준은 댓글의 라벨링이 fear면 -1, greed면 1로 판단합니다.<br/>\n",
    "+ 각 날짜별 모든 댓글의 fear/greed 점수를 평균을 낸 것을 그날의 fear/greed 점수로 합니다.<br/>\n",
    "점수 값은 -100 ~ 100 사이의 값을 가지게 되는데, 이를 0 ~ 100 사이의 값을 가지도록 조정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# 버트모델 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# KoBERT에 입력될 데이터셋 정리\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))  \n",
    "\n",
    "# fear/greed 평가\n",
    "def new_softmax(a) : \n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c) \n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = (exp_a / sum_exp_a) * 100\n",
    "    return np.round(y, 3)\n",
    "\n",
    "# 분류기 생성\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# 예측 모델 설정\n",
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            min_v = min(logits)\n",
    "            total = 0\n",
    "            probability = []\n",
    "            logits = np.round(new_softmax(logits), 3).tolist()\n",
    "            for logit in logits:\n",
    "                print(logit)\n",
    "                probability.append(np.round(logit, 3))\n",
    "\n",
    "            if np.argmax(logits) == 0: emotion = \"fear\"\n",
    "            elif np.argmax(logits) == 1: emotion = 'greed'\n",
    "\n",
    "            probability.append(emotion)\n",
    "            print(probability)\n",
    "    return probability\n",
    "\n",
    "\n",
    "# 파라미터 설정 \n",
    "max_len = 128\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 3\n",
    "max_grad_norm = 1\n",
    "log_interval = 100\n",
    "learning_rate =  0.0001\n",
    "\n",
    "\n",
    "## 학습 모델 로드\n",
    "model = torch.load('/KoBERT_model.pt')\n",
    "model.load_state_dict(torch.load('/model_state_dict.pt'))\n",
    "\n",
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "# 예측할 날짜 설정\n",
    "date_selection = 1\n",
    "\n",
    "# 데이터 불러오기\n",
    "target_date = [0,\n",
    "               '2022-06-01', '2022-06-02', '2022-06-03', '2022-06-04', '2022-06-05',\n",
    "               '2022-06-06', '2022-06-07', '2022-06-08', '2022-06-09', '2022-06-10',\n",
    "               '2022-06-11', '2022-06-12', '2022-06-13', '2022-06-14', '2022-06-15',\n",
    "               '2022-06-16', '2022-06-17', '2022-06-18', '2022-06-19', '2022-06-20',\n",
    "               '2022-06-21', '2022-06-22', '2022-06-23', '2022-06-24', '2022-06-25',\n",
    "               '2022-06-26', '2022-06-27', '2022-06-28', '2022-06-29', '2022-06-30']\n",
    "\n",
    "file_path = target_date[date_selection] + \".csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터 예측 실행\n",
    "df_pred = []\n",
    "for comment in df[\"정제된 댓글\"]:\n",
    "  fg_result = predict(comment)\n",
    "  score = abs(fg_result[0] - fg_result[1])\n",
    "  if fg_result[2] == \"fear\":\n",
    "    score = -score\n",
    "  df_pred.append(score)\n",
    "\n",
    "# 예측 결과를 csv 파일로 저장\n",
    "df[\"score\"] = df_pred\n",
    "file_path_2 = target_date[date_selection] + \"_score.csv\"\n",
    "df.to_csv(file_path_2)\n",
    "\n",
    "# 그날의 fear/greed 점수 산출\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622b870",
   "metadata": {},
   "source": [
    "## chapter 4. 시각화 및 서비스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3efc026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7a93d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bc543695463befc13840ec3d47f38ad407b1992c7ad047a9a3023a6d142e755"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
