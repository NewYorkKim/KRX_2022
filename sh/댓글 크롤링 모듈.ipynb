{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 댓글 크롤링 모듈\n",
    "+ 원본 댓글 가져오는 프로세스\n",
    "    + 각 종목별 종토방 댓글과 유튜브 댓글을 모두 취합하여, 하나의 리스트에 저장\n",
    "    + 리스트에 저장된 데이터프레임을 모두 전처리\n",
    "    + 전처리한 이후 날짜에 맞는 데이터만을 추려서 날짜 데이터 파일로 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 폴더구성(root_path)\n",
    "    + 크롤링 모듈 파일\n",
    "    + data \n",
    "        + code : krx 관련파일, 훈련데이터\n",
    "        + date : 날짜별 댓글 데이터\n",
    "        + score : 공포탐욕 점수\n",
    "        + youtube : 유튜브 댓글 데이터\n",
    "        + model : 예측에 사용될 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root path 설정\n",
    "root_path = \"C:/sh/study/krx데이콘/krx_2022/sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자 크롤링\n",
      "78페이지 크롤링 완료.\n",
      "NAVER 크롤링\n",
      "2페이지 크롤링 완료.\n",
      "삼성SDI 크롤링\n",
      "\n",
      "LG화학 크롤링\n",
      "\n",
      "카카오 크롤링\n",
      "1페이지 크롤링 완료.\n",
      "유한양행 크롤링\n",
      "\n",
      "SK텔레콤 크롤링\n",
      "\n",
      "POSCO홀딩스 크롤링\n",
      "1페이지 크롤링 완료.\n",
      "현대모비스 크롤링\n",
      "2페이지 크롤링 완료.\n",
      "SK이노베이션 크롤링\n",
      "\n",
      "삼성화재 크롤링\n",
      "\n",
      "롯데케미칼 크롤링\n",
      "\n",
      "현대차 크롤링\n",
      "2페이지 크롤링 완료.\n",
      "셀트리온 크롤링\n",
      "4페이지 크롤링 완료.\n",
      "삼성전기 크롤링\n",
      "1페이지 크롤링 완료.\n",
      "아모레퍼시픽 크롤링\n",
      "\n",
      "삼성물산 크롤링\n",
      "\n",
      "이마트 크롤링\n",
      "1페이지 크롤링 완료.\n",
      "CJ ENM 크롤링\n",
      "\n",
      "SK하이닉스 크롤링\n",
      "4페이지 크롤링 완료.\n",
      "한국조선해양 크롤링\n",
      "\n",
      "LG전자 크롤링\n",
      "1페이지 크롤링 완료.\n",
      "기아 크롤링\n",
      "\n",
      "넷마블 크롤링\n",
      "2페이지 크롤링 완료.\n",
      "삼성생명 크롤링\n",
      "\n",
      "KB금융 크롤링\n",
      "\n",
      "현대건설 크롤링\n",
      "\n",
      "신한지주 크롤링\n",
      "\n",
      "LG디스플레이 크롤링\n",
      "\n",
      "미래에셋증권 크롤링\n",
      "\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1871785 from 3181877 sents. mem=0.446 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=16231321, mem=4.391 Gb\n",
      "[Noun Extractor] batch prediction was completed for 460511 words\n",
      "[Noun Extractor] checked compounds. discovered 464165 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 411493 -> 313632\n",
      "[Noun Extractor] postprocessing ignore_features : 313632 -> 313101\n",
      "[Noun Extractor] postprocessing ignore_NJ : 313101 -> 307570\n",
      "[Noun Extractor] 307570 nouns (464165 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=5.210 Gb                    \n",
      "[Noun Extractor] 73.32 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "\n",
    "import datetime\n",
    "import scrapetube\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "import soynlp\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 종목코드 가져오는 코드\n",
    "def get_code(symbol):\n",
    "    krx = pd.read_csv(root_path + '/data/code/krx_code.csv',encoding='utf-8')\n",
    "    krx = krx.set_index('한글 종목약명')\n",
    "    try:\n",
    "        code = krx.at[symbol,'단축코드']\n",
    "        return code\n",
    "    except:\n",
    "        print('종목명을 다시 확인해주세요.')\n",
    "        return 0\n",
    "\n",
    "# 종토방 댓글 가져오는 코드\n",
    "def get_comment_csv(symbol,page,year,month,day):   \n",
    "    code = get_code(symbol)\n",
    "    date_list = [] # 날짜\n",
    "    comment_list = [] # 댓글\n",
    "    view_list = [] # 조회수\n",
    "    good_list = [] # 좋아요\n",
    "    bad_list = [] # 싫어요\n",
    "    flag = 0\n",
    "    for i in range(1,page+1):\n",
    "        url = f'https://finance.naver.com/item/board.naver?code={code}&page={i}'\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36 Edg/100.0.1185.50'}\n",
    "        res = requests.get(url, headers = headers)\n",
    "        bs = BeautifulSoup(res.text, 'html.parser')\n",
    "        for j in range(20):\n",
    "            try:\n",
    "                root = bs.find('div',{'class':'section inner_sub'}).find_all('tr',{'onmouseover':'mouseOver(this)'})[j].text.split('\\n')\n",
    "                \n",
    "                date_list.append(root[1].replace('.','-'))\n",
    "                \n",
    "                if len(root) == 14: # 답글\n",
    "                    comment_list.append('답글:'+root[4])\n",
    "                    view_list.append(root[10])\n",
    "                    good_list.append(root[11])\n",
    "                    bad_list.append(root[12])          \n",
    "                elif len(root) == 13: # 기본\n",
    "                    comment_list.append(root[3])\n",
    "                    view_list.append(root[9])\n",
    "                    good_list.append(root[10])\n",
    "                    bad_list.append(root[11])\n",
    "                else: # 에러\n",
    "                    comment_list.append('error')\n",
    "                    view_list.append(0)\n",
    "                    good_list.append(0)\n",
    "                    bad_list.append(0)   \n",
    "            except:\n",
    "                break\n",
    "            tp = [int(j) for j in root[1].split()[0].split('.')]\n",
    "            if dt.datetime(tp[0],tp[1],tp[2]) < dt.datetime(year,month,day):\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "        print(f'\\r{i}페이지 크롤링 완료.',end='')\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['날짜'] = date_list\n",
    "    df['댓글'] = comment_list\n",
    "    df['조회수'] = view_list\n",
    "    df['좋아요'] = good_list\n",
    "    df['싫어요'] = bad_list\n",
    "    return df\n",
    "\n",
    "# 종목이름 가져오는 코드\n",
    "def get_company_name():\n",
    "    df = pd.read_excel(root_path + '/data/code/KODEX_KTOP_30_20220629.xlsx',header=2).drop(0,axis=0)\n",
    "    return df.종목명.tolist()\n",
    "\n",
    "# 해당 년,월,일까지 종토방 댓글 가져오는 코드\n",
    "# 종목이름 순서대로 각 데이터프레임을 리스트에 저장하여 반환\n",
    "def get_date_comment(year,month,day):\n",
    "    c_list = get_company_name()\n",
    "    data_list = []\n",
    "    for company in c_list:\n",
    "        print(company,\"크롤링\")\n",
    "        df = get_comment_csv(company,10000,year,month,day)\n",
    "        data_list.append(df)\n",
    "        print()\n",
    "    return data_list\n",
    "\n",
    "# 유튜브 댓글 크롤링\n",
    "class YoutubeAPI():\n",
    "    def __init__ (self,date):\n",
    "        self.date = date\n",
    "        self.api_key = 'AIzaSyB-ttkB5mrZ6eo_iXlZdSv7zu105SgS2-E'\n",
    "        self.youtube = build('youtube', 'v3', developerKey=self.api_key)\n",
    "        self.channel_id = 'UChlv4GSd7OQl3js-jkLOnFA' # 삼프로TV\n",
    "\n",
    "        self.get_video_ids()\n",
    "        \n",
    "    def get_video_ids(self):\n",
    "        videos = scrapetube.get_channel(self.channel_id)\n",
    "        \n",
    "        video_ids = []\n",
    "        for video in videos:\n",
    "            video_ids.append(video['videoId'])\n",
    "        \n",
    "        (date, date_range) = self.get_date_input()\n",
    "        self.get_video_infos(video_ids, date, date_range)\n",
    "        \n",
    "    def get_date_input(self):\n",
    "        date = self.date\n",
    "        date_range = (0)\n",
    "        \n",
    "        return (date, date_range)\n",
    "    \n",
    "    def get_video_infos(self, video_ids, date, date_range):\n",
    "        video_infos = []\n",
    "        \n",
    "        for i in range(date_range + 1):\n",
    "            start = i * 50\n",
    "            end = (i + 1) * 50\n",
    "            \n",
    "            video_request = self.youtube.videos().list(\n",
    "                part='snippet',\n",
    "                id=','.join(video_ids[start:end]))\n",
    "            \n",
    "            video_response = video_request.execute()\n",
    "            \n",
    "            for item in video_response['items']:\n",
    "                title = item['snippet']['title']\n",
    "                if ('글로벌 이슈체크' in title) or ('글로벌 마켓브리핑' in title) or ('직장인 vlog' in title):\n",
    "                    continue\n",
    "                if date in item['snippet']['publishedAt'].split()[0]:\n",
    "                    video_infos.append([item['snippet']['title'], item['snippet']['publishedAt'], item['id']])\n",
    "    \n",
    "        df_ids = pd.DataFrame(video_infos, columns=['title', 'video_date', 'id'])\n",
    "            \n",
    "        self.get_comments(date, df_ids)\n",
    "            \n",
    "    def get_comments(self, date, df_ids):\n",
    "        comments = []\n",
    "        \n",
    "        for video_id in df_ids['id']:\n",
    "            api_obj = build('youtube', 'v3', developerKey=self.api_key)\n",
    "            response = api_obj.commentThreads().list(part='snippet', videoId=video_id, maxResults=100).execute()\n",
    "            \n",
    "            while response:\n",
    "                for item in response['items']:\n",
    "                    comment = item['snippet']['topLevelComment']['snippet']\n",
    "                    if date in comment['publishedAt'].split()[0]:\n",
    "                        comments.append([video_id, comment['textDisplay'], comment['authorDisplayName'], comment['publishedAt'], comment['likeCount']])\n",
    "            \n",
    "                if 'nextPageToken' in response:\n",
    "                    response = api_obj.commentThreads().list(part='snippet', videoId=video_id, pageToken=response['nextPageToken'], maxResults=100).execute()\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "        df_comments = pd.DataFrame(comments, columns=['id', 'comment', 'author', 'comment_date', 'num_likes'])\n",
    "        \n",
    "        df = pd.merge(df_comments, df_ids, on='id', how='outer')\n",
    "        \n",
    "        df.to_csv(root_path + f'/data/youtube/sampro_{date}.csv', index=False)\n",
    "        \n",
    "    def update_db(self, df):\n",
    "        pass\n",
    "    \n",
    "    def execute_daily(self):\n",
    "        pass\n",
    "\n",
    "# 특수문자 제거\n",
    "def clean_sents_df(target):\n",
    "    df = target\n",
    "    df['정제된 댓글'] = df['댓글'].str.replace('\\\\[삭제된 게시물의 답글\\\\]',' ')\n",
    "    df['정제된 댓글'] = df['정제된 댓글'].str.replace('답글:',' ')\n",
    "    df['정제된 댓글'] = df['정제된 댓글'].str.replace('[^가-힣]',' ').str.replace(' +',' ').str.strip()\n",
    "    df = df[df['정제된 댓글'] != '']\n",
    "    df = df.reset_index(drop=True)\n",
    "    return  df\n",
    "\n",
    "# 댓글 토큰화를 위한 말뭉치 준비\n",
    "def return_tokenizer():\n",
    "    corpus = DoublespaceLineCorpus(root_path + \"/data/code/corpus_target.txt\",iter_sent=True)\n",
    "    noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "    nouns = noun_extractor.train_extract(corpus)\n",
    "    scores = {word:score.score for word, score in nouns.items()}\n",
    "    tokenizer = LTokenizer(scores=scores)\n",
    "    return tokenizer\n",
    "\n",
    "# 오늘의 댓글 데이터를 저장 후 반환\n",
    "def get_data_list(y,m,d):\n",
    "    data_list = get_date_comment(y,m,d)\n",
    "    if len(str(m)) == 1:\n",
    "        m = f\"0{m}\"\n",
    "    if len(str(d)) == 1:\n",
    "        d = f\"0{d}\"\n",
    "    date = f\"{y}-{m}-{d}\"\n",
    "    ya = YoutubeAPI(date)\n",
    "    df = pd.read_csv(root_path + f\"/data/youtube/sampro_{date}.csv\")\n",
    "    df = df[[\"video_date\",\"comment\"]]\n",
    "    df.columns = [\"날짜\",\"댓글\"]\n",
    "    data_list.append(df)\n",
    "    return data_list\n",
    "\n",
    "# 각 데이터를 전처리 후 리스트에 저장\n",
    "def comment_prep(data_list):\n",
    "    tokenizer = return_tokenizer()\n",
    "\n",
    "    pp_list = []\n",
    "    for company_data in data_list:\n",
    "        target_df = clean_sents_df(company_data)\n",
    "        target_df['토큰화 댓글'] = [tokenizer(str(i)) for i in target_df['정제된 댓글']]\n",
    "        pp_list.append(target_df)\n",
    "    \n",
    "    for df in pp_list:\n",
    "        date_list = []\n",
    "        for i in range(len(df[\"날짜\"])):\n",
    "            date_list.append(df[\"날짜\"][i][:10])\n",
    "        df[\"날짜\"] = date_list\n",
    "    \n",
    "    return pp_list\n",
    "\n",
    "# 댓글 크롤링, 전처리, 파일로 저장\n",
    "def date_crawler(y,m,d):\n",
    "    data_list = get_data_list(y,m,d)\n",
    "    pp_list = comment_prep(data_list)\n",
    "    df_day = pp_list[0]\n",
    "    for i in range(1,len(pp_list)):\n",
    "        df_day = pd.concat([df_day, pp_list[i]])\n",
    "    if len(str(m)) == 1:\n",
    "        m = f\"0{m}\"\n",
    "    if len(str(d)) == 1:\n",
    "        d = f\"0{d}\"\n",
    "    today = f\"{y}-{m}-{d}\"\n",
    "    df_day = df_day[[\"날짜\",\"정제된 댓글\"]]\n",
    "    df = df_day[df_day[\"날짜\"] == today]\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(root_path + f\"/data/date/{today}.csv\")\n",
    "\n",
    "def get_today_ymd():\n",
    "    y = datetime.datetime.today().year\n",
    "    m = datetime.datetime.today().month\n",
    "    d = datetime.datetime.today().day\n",
    "    return y,m,d\n",
    "\n",
    "\n",
    "# 크롤링 실행\n",
    "y,m,d = get_today_ymd()\n",
    "date_crawler(y,m,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bc543695463befc13840ec3d47f38ad407b1992c7ad047a9a3023a6d142e755"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
