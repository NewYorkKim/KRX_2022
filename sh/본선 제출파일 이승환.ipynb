{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c713ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root path 설정\n",
    "root_path = \"C:/sh/study/krx데이콘/krx_2022/sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64745ceb",
   "metadata": {},
   "source": [
    "## 2022-06-01 ~ 2022-08-09 데이터 파일로 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d1e06",
   "metadata": {},
   "source": [
    "+ 모듈 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "\n",
    "import datetime\n",
    "import scrapetube\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "import soynlp\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b049ce7",
   "metadata": {},
   "source": [
    "+ raw data 크롤링 및 분할 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종목코드 가져오는 코드\n",
    "def get_code(symbol):\n",
    "    krx = pd.read_csv(root_path + '/data/code/krx_code.csv',encoding='utf-8')\n",
    "    krx = krx.set_index('한글 종목약명')\n",
    "    try:\n",
    "        code = krx.at[symbol,'단축코드']\n",
    "        return code\n",
    "    except:\n",
    "        print('종목명을 다시 확인해주세요.')\n",
    "        return 0\n",
    "\n",
    "# 종토방 댓글 가져오는 코드\n",
    "def get_comment_csv(symbol,page,year,month,day):   \n",
    "    code = get_code(symbol)\n",
    "    date_list = [] # 날짜\n",
    "    comment_list = [] # 댓글\n",
    "    view_list = [] # 조회수\n",
    "    good_list = [] # 좋아요\n",
    "    bad_list = [] # 싫어요\n",
    "    flag = 0\n",
    "    for i in range(1,page+1):\n",
    "        url = f'https://finance.naver.com/item/board.naver?code={code}&page={i}'\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36 Edg/100.0.1185.50'}\n",
    "        res = requests.get(url, headers = headers)\n",
    "        bs = BeautifulSoup(res.text, 'html.parser')\n",
    "        for j in range(20):\n",
    "            try:\n",
    "                root = bs.find('div',{'class':'section inner_sub'}).find_all('tr',{'onmouseover':'mouseOver(this)'})[j].text.split('\\n')\n",
    "                \n",
    "                date_list.append(root[1].replace('.','-'))\n",
    "                \n",
    "                if len(root) == 14: # 답글\n",
    "                    comment_list.append('답글:'+root[4])\n",
    "                    view_list.append(root[10])\n",
    "                    good_list.append(root[11])\n",
    "                    bad_list.append(root[12])          \n",
    "                elif len(root) == 13: # 기본\n",
    "                    comment_list.append(root[3])\n",
    "                    view_list.append(root[9])\n",
    "                    good_list.append(root[10])\n",
    "                    bad_list.append(root[11])\n",
    "                else: # 에러\n",
    "                    comment_list.append('error')\n",
    "                    view_list.append(0)\n",
    "                    good_list.append(0)\n",
    "                    bad_list.append(0)   \n",
    "            except:\n",
    "                break\n",
    "            tp = [int(j) for j in root[1].split()[0].split('.')]\n",
    "            if dt.datetime(tp[0],tp[1],tp[2]) < dt.datetime(year,month,day):\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "        print(f'\\r{i}페이지 크롤링 완료.',end='')\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['날짜'] = date_list\n",
    "    df['댓글'] = comment_list\n",
    "    df['조회수'] = view_list\n",
    "    df['좋아요'] = good_list\n",
    "    df['싫어요'] = bad_list\n",
    "    return df\n",
    "\n",
    "# 종목이름 가져오는 코드\n",
    "def get_company_name():\n",
    "    df = pd.read_excel(root_path + '/data/code/KODEX_KTOP_30_20220629.xlsx',header=2).drop(0,axis=0)\n",
    "    return df.종목명.tolist()\n",
    "\n",
    "# 해당 년,월,일까지 종토방 댓글 가져오는 코드\n",
    "# 종목이름 순서대로 각 데이터프레임을 리스트에 저장하여 반환\n",
    "def get_date_comment(year,month,day):\n",
    "    c_list = get_company_name()\n",
    "    data_list = []\n",
    "    for company in c_list:\n",
    "        print(company,\"크롤링\")\n",
    "        df = get_comment_csv(company,10000,year,month,day)\n",
    "        data_list.append(df)\n",
    "        print()\n",
    "    return data_list\n",
    "\n",
    "# 특수문자 제거\n",
    "def clean_sents_df(target):\n",
    "    df = target\n",
    "    df['정제된 댓글'] = df['댓글'].str.replace('\\\\[삭제된 게시물의 답글\\\\]',' ')\n",
    "    df['정제된 댓글'] = df['정제된 댓글'].str.replace('답글:',' ')\n",
    "    df['정제된 댓글'] = df['정제된 댓글'].str.replace('[^가-힣]',' ').str.replace(' +',' ').str.strip()\n",
    "    df = df[df['정제된 댓글'] != '']\n",
    "    df = df.reset_index(drop=True)\n",
    "    return  df\n",
    "\n",
    "# 댓글 토큰화를 위한 말뭉치 준비\n",
    "def return_tokenizer():\n",
    "    corpus = DoublespaceLineCorpus(root_path + \"/data/code/corpus_target.txt\",iter_sent=True)\n",
    "    noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "    nouns = noun_extractor.train_extract(corpus)\n",
    "    scores = {word:score.score for word, score in nouns.items()}\n",
    "    tokenizer = LTokenizer(scores=scores)\n",
    "    return tokenizer\n",
    "\n",
    "# 오늘의 댓글 데이터를 저장 후 반환\n",
    "def get_data_list():\n",
    "    # 2022-06-01 데이터부터 가져옴\n",
    "    data_list = get_date_comment(2022,6,1)\n",
    "    df = pd.read_csv(root_path + f\"/data/youtube/sampro.csv\")\n",
    "    data_list.append(df)\n",
    "    return data_list\n",
    "\n",
    "# 각 데이터를 전처리 후 리스트에 저장\n",
    "def comment_prep(data_list):\n",
    "    tokenizer = return_tokenizer()\n",
    "\n",
    "    pp_list = []\n",
    "    for company_data in data_list:\n",
    "        target_df = clean_sents_df(company_data)\n",
    "        target_df['토큰화 댓글'] = [tokenizer(str(i)) for i in target_df['정제된 댓글']]\n",
    "        pp_list.append(target_df)\n",
    "    \n",
    "    return pp_list\n",
    "\n",
    "# 댓글 크롤링, 전처리, 파일로 저장\n",
    "def date_crawler():\n",
    "    data_list = get_data_list()\n",
    "    pp_list = comment_prep(data_list)\n",
    "    df_day = pp_list[0]\n",
    "    for i in range(1,len(pp_list)):\n",
    "        df_day = pd.concat([df_day, pp_list[i]])\n",
    "    today = str(datetime.datetime.today())[:10]\n",
    "    df_day = df_day[[\"날짜\",\"정제된 댓글\"]]\n",
    "    df_day.dropna(inplace=True)\n",
    "    df_day.to_csv(root_path + f\"/data/alldf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 데이터 크롤링\n",
    "    + 전체 데이터를 하나의 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd804ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1956c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 날짜로 체크\n",
    "target_date = ['2022-06-01', '2022-06-02', '2022-06-03', '2022-06-04', '2022-06-05',\n",
    "               '2022-06-06', '2022-06-07', '2022-06-08', '2022-06-09', '2022-06-10',\n",
    "               '2022-06-11', '2022-06-12', '2022-06-13', '2022-06-14', '2022-06-15',\n",
    "               '2022-06-16', '2022-06-17', '2022-06-18', '2022-06-19', '2022-06-20',\n",
    "               '2022-06-21', '2022-06-22', '2022-06-23', '2022-06-24', '2022-06-25',\n",
    "               '2022-06-26', '2022-06-27', '2022-06-28', '2022-06-29', '2022-06-30',\n",
    "               '2022-07-01', '2022-07-02', '2022-07-03', '2022-07-04', '2022-07-05',\n",
    "               '2022-07-06', '2022-07-07', '2022-07-08', '2022-07-09', '2022-07-10',\n",
    "               '2022-07-11', '2022-07-12', '2022-07-13', '2022-07-14', '2022-07-15',\n",
    "               '2022-07-16', '2022-07-17', '2022-07-18', '2022-07-19', '2022-07-20',\n",
    "               '2022-07-21', '2022-07-22', '2022-07-23', '2022-07-24', '2022-07-25',\n",
    "               '2022-07-26', '2022-07-27', '2022-07-28', '2022-07-29', '2022-07-30',\"2022-07-31\",\n",
    "               '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05',\n",
    "               '2022-08-06', '2022-08-07', '2022-08-08', '2022-08-09']\n",
    "\n",
    "df = pd.read_csv(root_path + f\"/data/alldf.csv\")\n",
    "\n",
    "# 날짜 형식을 동일하게 변경\n",
    "date_list = []\n",
    "for i in range(len(df[\"날짜\"])):\n",
    "    date_list.append(str(df[\"날짜\"].iloc[i][:10]))\n",
    "df[\"날짜\"] = date_list\n",
    "\n",
    "# 각 날짜별 파일로 변경\n",
    "for date in target_date:\n",
    "    df2 = df[df[\"날짜\"]==date]\n",
    "    df2.to_csv(root_path + f\"/data/date/{date}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf9554",
   "metadata": {},
   "source": [
    "+ 날짜별 파일 저장\n",
    "    + 하나의 파일로 저장된 데이터를 날짜별 데이터로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 날짜로 체크\n",
    "target_date = ['2022-06-01', '2022-06-02', '2022-06-03', '2022-06-04', '2022-06-05',\n",
    "               '2022-06-06', '2022-06-07', '2022-06-08', '2022-06-09', '2022-06-10',\n",
    "               '2022-06-11', '2022-06-12', '2022-06-13', '2022-06-14', '2022-06-15',\n",
    "               '2022-06-16', '2022-06-17', '2022-06-18', '2022-06-19', '2022-06-20',\n",
    "               '2022-06-21', '2022-06-22', '2022-06-23', '2022-06-24', '2022-06-25',\n",
    "               '2022-06-26', '2022-06-27', '2022-06-28', '2022-06-29', '2022-06-30',\n",
    "               '2022-07-01', '2022-07-02', '2022-07-03', '2022-07-04', '2022-07-05',\n",
    "               '2022-07-06', '2022-07-07', '2022-07-08', '2022-07-09', '2022-07-10',\n",
    "               '2022-07-11', '2022-07-12', '2022-07-13', '2022-07-14', '2022-07-15',\n",
    "               '2022-07-16', '2022-07-17', '2022-07-18', '2022-07-19', '2022-07-20',\n",
    "               '2022-07-21', '2022-07-22', '2022-07-23', '2022-07-24', '2022-07-25',\n",
    "               '2022-07-26', '2022-07-27', '2022-07-28', '2022-07-29', '2022-07-30',\"2022-07-31\",\n",
    "               '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05',\n",
    "               '2022-08-06', '2022-08-07', '2022-08-08', '2022-08-09']\n",
    "\n",
    "df = pd.read_csv(root_path + f\"/data/alldf.csv\")\n",
    "\n",
    "for date in target_date:\n",
    "    df2 = df[df[\"날짜\"]==date]\n",
    "    df2.to_csv(root_path + f\"/data/date/{date}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11874d66",
   "metadata": {},
   "source": [
    "## 하루치 데이터 크롤링\n",
    "+ 날짜 정보를 받아 데이터를 크롤링하는 모듈\n",
    "+ 날짜는 컴퓨터에 내장된 시계에서 받아서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "\n",
    "import datetime\n",
    "import scrapetube\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "import soynlp\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종목코드 가져오는 코드\n",
    "def get_code(symbol):\n",
    "    krx = pd.read_csv(root_path + '/data/code/krx_code.csv',encoding='utf-8')\n",
    "    krx = krx.set_index('한글 종목약명')\n",
    "    try:\n",
    "        code = krx.at[symbol,'단축코드']\n",
    "        return code\n",
    "    except:\n",
    "        print('종목명을 다시 확인해주세요.')\n",
    "        return 0\n",
    "\n",
    "# 종토방 댓글 가져오는 코드\n",
    "def get_comment_csv(symbol,page,year,month,day):   \n",
    "    code = get_code(symbol)\n",
    "    date_list = [] # 날짜\n",
    "    comment_list = [] # 댓글\n",
    "    view_list = [] # 조회수\n",
    "    good_list = [] # 좋아요\n",
    "    bad_list = [] # 싫어요\n",
    "    flag = 0\n",
    "    for i in range(1,page+1):\n",
    "        url = f'https://finance.naver.com/item/board.naver?code={code}&page={i}'\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36 Edg/100.0.1185.50'}\n",
    "        res = requests.get(url, headers = headers)\n",
    "        bs = BeautifulSoup(res.text, 'html.parser')\n",
    "        for j in range(20):\n",
    "            try:\n",
    "                root = bs.find('div',{'class':'section inner_sub'}).find_all('tr',{'onmouseover':'mouseOver(this)'})[j].text.split('\\n')\n",
    "                \n",
    "                date_list.append(root[1].replace('.','-'))\n",
    "                \n",
    "                if len(root) == 14: # 답글\n",
    "                    comment_list.append('답글:'+root[4])\n",
    "                    view_list.append(root[10])\n",
    "                    good_list.append(root[11])\n",
    "                    bad_list.append(root[12])          \n",
    "                elif len(root) == 13: # 기본\n",
    "                    comment_list.append(root[3])\n",
    "                    view_list.append(root[9])\n",
    "                    good_list.append(root[10])\n",
    "                    bad_list.append(root[11])\n",
    "                else: # 에러\n",
    "                    comment_list.append('error')\n",
    "                    view_list.append(0)\n",
    "                    good_list.append(0)\n",
    "                    bad_list.append(0)   \n",
    "            except:\n",
    "                break\n",
    "            tp = [int(j) for j in root[1].split()[0].split('.')]\n",
    "            if dt.datetime(tp[0],tp[1],tp[2]) < dt.datetime(year,month,day):\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1:\n",
    "            break\n",
    "        print(f'\\r{i}페이지 크롤링 완료.',end='')\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['날짜'] = date_list\n",
    "    df['댓글'] = comment_list\n",
    "    df['조회수'] = view_list\n",
    "    df['좋아요'] = good_list\n",
    "    df['싫어요'] = bad_list\n",
    "    return df\n",
    "\n",
    "# 종목이름 가져오는 코드\n",
    "def get_company_name():\n",
    "    df = pd.read_excel(root_path + '/data/code/KODEX_KTOP_30_20220629.xlsx',header=2).drop(0,axis=0)\n",
    "    return df.종목명.tolist()\n",
    "\n",
    "# 해당 년,월,일까지 종토방 댓글 가져오는 코드\n",
    "# 종목이름 순서대로 각 데이터프레임을 리스트에 저장하여 반환\n",
    "def get_date_comment(year,month,day):\n",
    "    c_list = get_company_name()\n",
    "    data_list = []\n",
    "    for company in c_list:\n",
    "        print(company,\"크롤링\")\n",
    "        df = get_comment_csv(company,10000,year,month,day)\n",
    "        data_list.append(df)\n",
    "        print()\n",
    "    return data_list\n",
    "\n",
    "# 유튜브 댓글 크롤링\n",
    "class YoutubeAPI():\n",
    "    def __init__ (self,date):\n",
    "        self.date = date\n",
    "        self.api_key = 'AIzaSyB-ttkB5mrZ6eo_iXlZdSv7zu105SgS2-E'\n",
    "        self.youtube = build('youtube', 'v3', developerKey=self.api_key)\n",
    "        self.channel_id = 'UChlv4GSd7OQl3js-jkLOnFA' # 삼프로TV\n",
    "\n",
    "        self.get_video_ids()\n",
    "        \n",
    "    def get_video_ids(self):\n",
    "        videos = scrapetube.get_channel(self.channel_id)\n",
    "        \n",
    "        video_ids = []\n",
    "        for video in videos:\n",
    "            video_ids.append(video['videoId'])\n",
    "        \n",
    "        (date, date_range) = self.get_date_input()\n",
    "        self.get_video_infos(video_ids, date, date_range)\n",
    "        \n",
    "    def get_date_input(self):\n",
    "        date = self.date\n",
    "        date_range = (0)\n",
    "        \n",
    "        return (date, date_range)\n",
    "    \n",
    "    def get_video_infos(self, video_ids, date, date_range):\n",
    "        video_infos = []\n",
    "        \n",
    "        for i in range(date_range + 1):\n",
    "            start = i * 50\n",
    "            end = (i + 1) * 50\n",
    "            \n",
    "            video_request = self.youtube.videos().list(\n",
    "                part='snippet',\n",
    "                id=','.join(video_ids[start:end]))\n",
    "            \n",
    "            video_response = video_request.execute()\n",
    "            \n",
    "            for item in video_response['items']:\n",
    "                title = item['snippet']['title']\n",
    "                if ('글로벌 이슈체크' in title) or ('글로벌 마켓브리핑' in title) or ('직장인 vlog' in title):\n",
    "                    continue\n",
    "                if date in item['snippet']['publishedAt'].split()[0]:\n",
    "                    video_infos.append([item['snippet']['title'], item['snippet']['publishedAt'], item['id']])\n",
    "    \n",
    "        df_ids = pd.DataFrame(video_infos, columns=['title', 'video_date', 'id'])\n",
    "            \n",
    "        self.get_comments(date, df_ids)\n",
    "            \n",
    "    def get_comments(self, date, df_ids):\n",
    "        comments = []\n",
    "        \n",
    "        for video_id in df_ids['id']:\n",
    "            api_obj = build('youtube', 'v3', developerKey=self.api_key)\n",
    "            response = api_obj.commentThreads().list(part='snippet', videoId=video_id, maxResults=100).execute()\n",
    "            \n",
    "            while response:\n",
    "                for item in response['items']:\n",
    "                    comment = item['snippet']['topLevelComment']['snippet']\n",
    "                    if date in comment['publishedAt'].split()[0]:\n",
    "                        comments.append([video_id, comment['textDisplay'], comment['authorDisplayName'], comment['publishedAt'], comment['likeCount']])\n",
    "            \n",
    "                if 'nextPageToken' in response:\n",
    "                    response = api_obj.commentThreads().list(part='snippet', videoId=video_id, pageToken=response['nextPageToken'], maxResults=100).execute()\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "        df_comments = pd.DataFrame(comments, columns=['id', 'comment', 'author', 'comment_date', 'num_likes'])\n",
    "        \n",
    "        df = pd.merge(df_comments, df_ids, on='id', how='outer')\n",
    "        \n",
    "        df.to_csv(root_path + f'/data/youtube/sampro_{date}.csv', index=False)\n",
    "        \n",
    "    def update_db(self, df):\n",
    "        pass\n",
    "    \n",
    "    def execute_daily(self):\n",
    "        pass\n",
    "\n",
    "# 특수문자 제거\n",
    "def clean_sents_df(target):\n",
    "    df = target\n",
    "    df['정제된 댓글'] = df['댓글'].str.replace('\\\\[삭제된 게시물의 답글\\\\]',' ')\n",
    "    df['정제된 댓글'] = df['정제된 댓글'].str.replace('답글:',' ')\n",
    "    df['정제된 댓글'] = df['정제된 댓글'].str.replace('[^가-힣]',' ').str.replace(' +',' ').str.strip()\n",
    "    df = df[df['정제된 댓글'] != '']\n",
    "    df = df.reset_index(drop=True)\n",
    "    return  df\n",
    "\n",
    "# 댓글 토큰화를 위한 말뭉치 준비\n",
    "def return_tokenizer():\n",
    "    corpus = DoublespaceLineCorpus(root_path + \"/data/code/corpus_target.txt\",iter_sent=True)\n",
    "    noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "    nouns = noun_extractor.train_extract(corpus)\n",
    "    scores = {word:score.score for word, score in nouns.items()}\n",
    "    tokenizer = LTokenizer(scores=scores)\n",
    "    return tokenizer\n",
    "\n",
    "# 오늘의 댓글 데이터를 저장 후 반환\n",
    "def get_data_list(y,m,d):\n",
    "    data_list = get_date_comment(y,m,d)\n",
    "    if len(str(m)) == 1:\n",
    "        m = f\"0{m}\"\n",
    "    if len(str(d)) == 1:\n",
    "        d = f\"0{d}\"\n",
    "    date = f\"{y}-{m}-{d}\"\n",
    "    ya = YoutubeAPI(date)\n",
    "    df = pd.read_csv(root_path + f\"/data/youtube/sampro_{date}.csv\")\n",
    "    df = df[[\"video_date\",\"comment\"]]\n",
    "    df.columns = [\"날짜\",\"댓글\"]\n",
    "    data_list.append(df)\n",
    "    return data_list\n",
    "\n",
    "# 각 데이터를 전처리 후 리스트에 저장\n",
    "def comment_prep(data_list):\n",
    "    tokenizer = return_tokenizer()\n",
    "\n",
    "    pp_list = []\n",
    "    for company_data in data_list:\n",
    "        target_df = clean_sents_df(company_data)\n",
    "        target_df['토큰화 댓글'] = [tokenizer(str(i)) for i in target_df['정제된 댓글']]\n",
    "        pp_list.append(target_df)\n",
    "    \n",
    "    for df in pp_list:\n",
    "        date_list = []\n",
    "        for i in range(len(df[\"날짜\"])):\n",
    "            date_list.append(df[\"날짜\"][i][:10])\n",
    "        df[\"날짜\"] = date_list\n",
    "    \n",
    "    return pp_list\n",
    "\n",
    "# 댓글 크롤링, 전처리, 파일로 저장\n",
    "def date_crawler(y,m,d):\n",
    "    data_list = get_data_list(y,m,d)\n",
    "    pp_list = comment_prep(data_list)\n",
    "    df_day = pp_list[0]\n",
    "    for i in range(1,len(pp_list)):\n",
    "        df_day = pd.concat([df_day, pp_list[i]])\n",
    "    if len(str(m)) == 1:\n",
    "        m = f\"0{m}\"\n",
    "    if len(str(d)) == 1:\n",
    "        d = f\"0{d}\"\n",
    "    today = f\"{y}-{m}-{d}\"\n",
    "    df_day = df_day[[\"날짜\",\"정제된 댓글\"]]\n",
    "    df = df_day[df_day[\"날짜\"] == today]\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(root_path + f\"/data/date/{today}.csv\")\n",
    "\n",
    "def get_today_ymd():\n",
    "    y = datetime.datetime.today().year\n",
    "    m = datetime.datetime.today().month\n",
    "    d = datetime.datetime.today().day\n",
    "    return y,m,d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4f2693",
   "metadata": {},
   "source": [
    "+ 댓글 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 실행\n",
    "y,m,d = get_today_ymd()\n",
    "date_crawler(y,m,d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2df44",
   "metadata": {},
   "source": [
    "## 버트모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9cfb4",
   "metadata": {},
   "source": [
    "+ 구글 드라이브 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373075c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google colab 패키지 사용\n",
    "from google.colab import drive\n",
    "\n",
    "# 구글 드라이브 연결\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f9d69",
   "metadata": {},
   "source": [
    "<!-- !pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece==0.1.96\n",
    "!pip install transformers==3.0.2\n",
    "!pip install torch\n",
    "!pip install gluonnlp\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "!pip install pykrx\n",
    "!pip install soynlp\n",
    "!pip install xlrd==1.2.0 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138af67",
   "metadata": {},
   "source": [
    "+ 모듈 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22202654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684e933",
   "metadata": {},
   "source": [
    "+ GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02658562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09dce6d",
   "metadata": {},
   "source": [
    "+ 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce834e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터 적재\n",
    "file_path2 = f\"{root_path}/data/code/train_label_-1_1.csv\"\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# -1인 라벨값을 0으로 변경\n",
    "for i in range(len(df2[\"label\"])):\n",
    "  if df2[\"label\"].iloc[i] == -1:\n",
    "    df2[\"label\"].iloc[i] = 0\n",
    "df2[\"label\"].astype(\"int32\")\n",
    "\n",
    "# 트레인 테스트 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(df2[\"정제된 댓글\"],df2[\"label\"],test_size=0.2,random_state=11)\n",
    "df_train = pd.concat([x_train,y_train],axis = 1)\n",
    "df_test = pd.concat([x_test,y_test],axis = 1)\n",
    "df_train.head()\n",
    "\n",
    "# 파라미터 설정 \n",
    "max_len = 128\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 2\n",
    "max_grad_norm = 1\n",
    "log_interval = 100\n",
    "learning_rate =  0.00001\n",
    "\n",
    "# 데이터셋을 koBERT 모델에 맞게 변형하는 클래스\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i]) for i in dataset[sent_idx]]\n",
    "        self.labels = [np.int32(i) for i in dataset[label_idx]]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))  \n",
    "\n",
    "\n",
    "# koBERT 모듈을 적재하여 모델을 학습시키는 분류기 \n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "# 훈련데이터, 테스트데이터 전처리\n",
    "data_train = BERTDataset(df_train, \"정제된 댓글\", \"label\", tok, max_len, True, False)\n",
    "data_test = BERTDataset(df_test, \"정제된 댓글\", \"label\", tok, max_len, True, False)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n",
    "\n",
    "# 모델 생성\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "\n",
    "# optimizer 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 모델 학습\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "train_dataloader\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "\n",
    "# 모델 저장\n",
    "# torch.save(model, root_path +\"/data/model/\"+'라벨_2_dr_0.5.pt') \n",
    "# torch.save(model.state_dict(),root_path +\"/data/model/\"+ 'model_state_dict_라벨_2_dr_0.5.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc6311",
   "metadata": {},
   "source": [
    "+ 날짜별 데이터 공포/탐욕 점수 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트모델 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# 파라미터 설정 \n",
    "max_len = 128\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 2\n",
    "max_grad_norm = 1\n",
    "log_interval = 100\n",
    "learning_rate =  0.00001\n",
    "\n",
    "# KoBERT에 입력될 데이터셋 정리\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))  \n",
    "\n",
    "# fear/greed 평가\n",
    "def new_softmax(a) : \n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c) \n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = (exp_a / sum_exp_a) * 100\n",
    "    return np.round(y, 3)\n",
    "\n",
    "# 분류기 생성\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# 예측 모델 설정\n",
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            min_v = min(logits)\n",
    "            total = 0\n",
    "            probability = []\n",
    "            logits = np.round(new_softmax(logits), 3).tolist()\n",
    "            for logit in logits:\n",
    "                probability.append(np.round(logit, 3))\n",
    "\n",
    "            if np.argmax(logits) == 0: emotion = \"fear\"\n",
    "            elif np.argmax(logits) == 1: emotion = 'greed'\n",
    "\n",
    "            probability.append(emotion)\n",
    "    return probability\n",
    "\n",
    "## 학습 모델 로드\n",
    "model = torch.load(root_path +\"/data/model/\"+'라벨_2_dr_0.5.pt')\n",
    "model.load_state_dict(torch.load(root_path +\"/data/model/\"+'model_state_dict_라벨_2_dr_0.5.pt'))\n",
    "\n",
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 예측\n",
    "target_date = ['2022-06-01', '2022-06-02', '2022-06-03', '2022-06-04', '2022-06-05',\n",
    "               '2022-06-06', '2022-06-07', '2022-06-08', '2022-06-09', '2022-06-10',\n",
    "               '2022-06-11', '2022-06-12', '2022-06-13', '2022-06-14', '2022-06-15',\n",
    "               '2022-06-16', '2022-06-17', '2022-06-18', '2022-06-19', '2022-06-20',\n",
    "               '2022-06-21', '2022-06-22', '2022-06-23', '2022-06-24', '2022-06-25',\n",
    "               '2022-06-26', '2022-06-27', '2022-06-28', '2022-06-29', '2022-06-30',\n",
    "               '2022-07-01', '2022-07-02', '2022-07-03', '2022-07-04', '2022-07-05',\n",
    "               '2022-07-06', '2022-07-07', '2022-07-08', '2022-07-09', '2022-07-10',\n",
    "               '2022-07-11', '2022-07-12', '2022-07-13', '2022-07-14', '2022-07-15',\n",
    "               '2022-07-16', '2022-07-17', '2022-07-18', '2022-07-19', '2022-07-20',\n",
    "               '2022-07-21', '2022-07-22', '2022-07-23', '2022-07-24', '2022-07-25',\n",
    "               '2022-07-26', '2022-07-27', '2022-07-28', '2022-07-29', '2022-07-30',\"2022-07-31\",\n",
    "               '2022-08-01', '2022-08-02', '2022-08-03', '2022-08-04', '2022-08-05',\n",
    "               '2022-08-06', '2022-08-07', '2022-08-08', '2022-08-09', '2022-08-10',\n",
    "               '2022-08-11', '2022-08-12', '2022-08-13', '2022-08-14', '2022-08-15',\n",
    "               '2022-08-16', '2022-08-17', '2022-08-18']\n",
    "\n",
    "for date in target_date:\n",
    "  try:\n",
    "    try:  # score 파일이 있는 경우 스킵\n",
    "      pd.read_csv((root_path + f\"/data/score/{date}_score.csv\"))\n",
    "      print(f\"{date} 점수 예측 완료\")\n",
    "      continue\n",
    "    except: # score 파일이 없는 경우 예측 진행\n",
    "      df = pd.read_csv(root_path + f\"/data/date/{date}.csv\")\n",
    "      print(f\"{date} 예측 진행중\")\n",
    "      df_pred = []\n",
    "      i = 0\n",
    "      for comment in df[\"정제된 댓글\"]:\n",
    "        i += 1\n",
    "        print(f'\\r{i}/{len(df[\"정제된 댓글\"])} 완료.',end='')\n",
    "        fg_result = predict(comment)\n",
    "        score = abs(fg_result[0] - fg_result[1])\n",
    "        if fg_result[2] == \"fear\":\n",
    "            score = -score\n",
    "        df_pred.append(score)\n",
    "        df2 = pd.DataFrame(df_pred,columns = [\"socre\"])\n",
    "        df2.to_csv(root_path + f\"/data/score/{date}_score.csv\")\n",
    "      print()\n",
    "  except:\n",
    "    # date 파일이 둘 다 없는 경우\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bc543695463befc13840ec3d47f38ad407b1992c7ad047a9a3023a6d142e755"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
